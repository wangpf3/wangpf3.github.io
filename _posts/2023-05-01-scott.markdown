---
layout: post
title:  "SCOTT: Self-Consistent Chain-of-Thought Distillation"
date:   2023-05-01 22:21:59 +00:00
image: /images/scott.png
categories: research
authors: "<strong>Peifeng Wang</strong>, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren"
venue: "Annual Meeting of the Association for Computational Linguistics"
arxiv: https://arxiv.org/abs/2305.01879
award: Outstanding Paper Award
code: https://github.com/wangpf3/consistent-CoT-distillation.git
blog: https://www.amazon.science/blog/teaching-language-models-to-reason-consistently
---
A faithful knowledge distillation method that learns a small, self-consistent Chain-of-Thought model from a large teacher model.
